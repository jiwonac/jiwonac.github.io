<!DOCTYPE html>
<html lang="en">
    <head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" href="/style.css">
    <title>Jiwon's Alcove | Understanding the LazyLazy Algorithm for Submodular Maximization</title>
    <meta name="description" content="Musings on computer science and life.">
    <!--mathjax-->
    <script type="text/javascript" id="MathJax-script" async
      src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
    </script>
    <!--use dollar signs for math-->
    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']]
        }
      };
  </script>
</head>
</html>
<body>
    <div id="body-container">
        <div id="body-grid">
            <div id="body-left">
                <h1><a href="/">Jiwon's Alcove</a></h1>
                
                    <div class="content">
                        <div class="content-left">
                            Jul 10, 2023<br>
                            
                        </div>
                        <div class="content-right">
                            <h2>Understanding the LazyLazy Algorithm for Submodular Maximization</h2>
                            
                            
                            <h3>Motivation</h3>
<p>Submodularity is the discrete analog of concavity and models diminishing returns. Intuitively, it means that adding an item to a collection early is more beneficial than adding it later.</p>
<p>Formally, let $V$ be a universe of of items and let $2^V$ be its powerset. The function $f: 2^V \to \mathbb{R}$ is submodular iff<br>
$$f(S \cup {v}) - f(S) \ge f(T \cup {v}) - f(T)$$<br>
for all $S \subseteq T \subseteq \Omega$ and $v \in V \setminus T$. This is equivalent to<br>
$$f(S) + f(T) \ge f(S \cup T) + f(S \cap T)$$<br>
for all $S, T \subseteq V$.</p>
<p>Submodular maximization is an incredibly useful model for many problems in real life, where we must select a subset of available items that exhibit diminishing returns. For instance, the problem of grocery shopping under a budget to maximize enjoyment could be modelled as submodular optimization under a knapsack constraint. This is because adding apples to the grocery haul is less beneficial when our bag is already full of similar items like pears.</p>
<p>A remarkable array of computer science problems can be reduced to submodular maximization. The following are but a subset of problems that can be modelled as such.</p>
<ol>
<li>Facility location problem</li>
<li>Set (multi-)cover problem</li>
<li>Vertex cover and tuple domination problem</li>
<li>k-medoids clustering</li>
<li>Robust sensor placement</li>
<li>Feature selection</li>
<li>Active learning</li>
</ol>
<p>Like all of these problems, submodular maximization is NP-Complete, and thus admits no exact polynomial time algorithm unless $P=NP$ or some moral equivalent.</p>
<p>Fortunately, submodular maximization under cardinality constraint can be approximated by a constant factor of $1 - 1/e$ with $O(nk)$ query complexity by a greedy algorithm, where $n = |V|$ and $k$ is the cardinality constraint. Query complexity stands for the number of times the function $f(S)$ must be evaluated.</p>
<p>Unfortunately, with a never-ending appetite for big data, $O(nk)$ time complexity leaves much to be desired, and a linear $O(n)$ query complexity would be ideal. In fact, it is impossible to guarantee more than $1 - 1/e - \epsilon$ approximation factor in expectation with less than $O(n)$ query complexity.</p>
<p>The holy grail of $O(n)$ query complexity, $1 - 1/e - \epsilon$ approximation was achieved by Mirzasoleiman et al. in their 2015 paper &quot;Lazier than Lazy Greedy.&quot; They called this algorithm <code>StochasticGreedy</code>, though I personally prefer the colloquial name <code>LazyLazy</code>.</p>
<p>Suppose $A$ is the collection that we have so far. In each iteration, <code>LazyLazy</code> samples $s = (n/k)\log (1 / \epsilon)$ candidate items from $V \setminus A$ uniformly randomly with replacement to obtain a candidate set $R$. They then choose the item in $R$ which greedily maximizes marginal gains, denoted $\Delta(a | A)$. This item $a_i$ is added to $A$. This process is repeated $k$ times.</p>
<h3>Theory</h3>
<p>Theoretical analysis behind the <code>LazyLazy</code> is nontrivial and at times unintuitive. In this post, I will try my best to explain the analysis as simply as possible.</p>
<p><em>Proof sketch.</em> The main gist of the proof is that adding a random element from $A^*$ to $A$ is optimal. Why is that? Suppose we have an optimal algorithm. It doesn't matter what order it collects $A^*$ so as long as it ends up with $A^*$ after $k$ iterations. This is because the set function $f(A^*)$ only cares about what $A^*$ is and not how it was obtained.</p>
<p>Thus, an algorithm which adds a random point from $A^* \setminus A$ to $A$ in each iteration is optimal. This magic algorithm does not exist. However, if an algorithm exists where it adds a random point from $A^* \setminus A$ to $A$ with high probability then it could have a good approximation factor. Furthermore, if it adds <em>any</em> point in each iteration with at least that much marginal gain, then the same approximation factor holds.</p>
<p><strong>Lemma.</strong> Given a current solution $A$, the expected gain of <code>LazyLazy</code> in one step is at least $\frac{1 - \epsilon}{k} \sum_{a \in A^* \setminus A} \Delta(a | A)$.</p>
<p><em>Proof.</em> We first bound the probability that $R \cap (A^* \setminus A) \neq \emptyset$.</p>
<p>Since $R$ consists of $s = \frac{n}{k}\log \frac{1}{\epsilon}$ random samples from $A \setminus A$,</p>
<p>$$<br>
\begin{align*}<br>
Pr[R \cap (A^* \setminus A) = \emptyset]<br>
&amp;= \left(1 - Pr[\text{one sample is in } A^* \setminus A]\right)^s\\<br>
&amp;= \left( 1 - \frac{|A^* \setminus A|}{|V \setminus A|} \right)^s\<br>
\end{align*}<br>
$$</p>
<p>Here, we take a detour and prove an inequality closely related to the Bernoulli's inequality: $(1 - t)^s \le e^{-st}$ for $s, t &gt; 0$.</p>
<p>Our starting point is $1 - t \le e^{-t}$ for $t \ge 0$. This can be shown via differentiation or more informally by graphing.</p>
<p><img src="/assets/blog/1-t_le_e-t.png" alt=""></p>
<p>$$<br>
\begin{align*}<br>
1 - t &amp;\le e^{-t}\\<br>
\ln(1 - t) &amp;\le \ln(e^{-t}) &amp;\text{by taking log}\\<br>
\ln(1 - t) &amp;\le -t\\<br>
s\ln(1 - t) &amp;\le -st &amp;\text{since } s &gt; 0\\<br>
\ln(1-t)^s &amp;\le \ln(e^{-st}) &amp;\text{by log definition}\\<br>
\therefore (1-t)^s &amp;\le e^{-st} &amp;\text{by taking exponent}<br>
\end{align*}<br>
$$</p>
<p>Since $\frac{A^* \setminus A}{V \setminus A} \ge 0$, and clearly $s &gt; 0$, so we can apply our result to simplify the inequality.</p>
<p>$$<br>
\begin{align*}<br>
Pr[R \cap (A^* \setminus A) = \emptyset] &amp;\le \left(1 - \frac{|A^* \setminus A|}{|V \setminus A|}\right)^s\\<br>
&amp;\le e^{-s\frac{|A^* \setminus A|}{|V \setminus A|}} &amp;\text{by inequality above}\\<br>
&amp;\le e^{-\frac{s}{n}|A^* \setminus A|} &amp;\text{since } \frac{|V \setminus A|}{n} \le 1\\<br>
\end{align*}<br>
$$</p>
<p>Since we're interested in the probability that $R$ and $A^* \setminus A$ does <em>not</em> overlap, inverse the inequality.</p>
<p>$$<br>
\begin{align*}<br>
Pr[R \cap (A^* \setminus A) \neq \emptyset] &amp;\ge 1 - e^{-\frac{s}{n}|A^* \setminus A|}\\<br>
&amp;\ge \frac{|A^* \setminus A|}{k} - e^{-\frac{s}{n}|A^* \setminus A|} &amp;\text{since } \frac{|A^* \setminus A|}{k} \le 1\\<br>
&amp;\ge \frac{|A^* \setminus A|}{k} - e^{-\frac{sk}{n}\frac{|A^* \setminus A|}{k}}\\<br>
\end{align*}<br>
$$</p>
<p>Here, we make use of the fact that $xe^{-\frac{sk}{n}} \le e^{-\frac{sk}{n}x}$ for $x \le 1$. This is because $xe^{-\frac{sk}{n}}$ is monotonically increasing and $e^{-\frac{sk}{n}x}$ is monotonically decreasing with an intersection when $x = 1$. Since $\frac{|A^* \setminus A|}{k} \le 1$,</p>
<p>$$<br>
\begin{align*}<br>
Pr[R \cap (A^* \setminus A) \neq \emptyset] &amp;\ge \frac{|A^* \setminus A|}{k} - \frac{|A^* \setminus A|}{k}e^{-\frac{sk}{n}}\\<br>
&amp;= \frac{|A^* \setminus A|}{k}\left(1 - e^{-\frac{s}{n}}\right).<br>
\end{align*}<br>
$$</p>
<p>Now substitute $s = \frac{n}{k}\log \frac{1}{\epsilon}$, then<br>
$$Pr[R \cap (A^* \setminus A) \neq \emptyset] \ge (1 - \epsilon)\frac{|A^* \setminus A|}{k}.$$</p>
<p>If $R$ and $A^* \setminus A$ overlap, then the greedy element in $R$ must be at least as good as a random element from $R \cap (A^* \setminus A)$. This is trivially true, since the greedy element in $R$ is at least as good a <em>any</em> element in $R$.</p>
<p>Now consider $\mathbb{E}[\Delta(a|A)]$, the marginal gain in this iteration. In the worst case, any element in $R \setminus (A^* \setminus A)$ may have zero marginal gain.</p>
<p>$$\mathbb{E}[\Delta(a|A)] \ge Pr[R \cap (A^*\setminus A) \neq \emptyset] \times \frac{1}{|A^* \setminus A|}\sum_{a \in A^*\setminus A}\Delta(a|A)$$</p>
<p>Here, $\frac{1}{|A^* \setminus A|}\sum_{a \in A^*\setminus A}\Delta(a|A)$ is the expected gain from a random element in $A^* \setminus A$.</p>
<p>By substitution of probability,</p>
<p>$$<br>
\begin{align*}<br>
\mathbb{E}[\Delta(a|A)] &amp;\ge (1 - \epsilon)\frac{|A^* \setminus A|}{k} \times \frac{1}{|A^* \setminus A|}\sum_{a \in A^*\setminus A}\Delta(a|A)\\<br>
&amp;= \frac{1 - \epsilon}{k}\sum_{a \in A^* \setminus A} \Delta(a|A).<br>
\end{align*}<br>
$$</p>
<p>An equivalent definition of submodularity states that<br>
$$\sum_{v \in T \setminus S} \Delta(v|S) \ge f(T) - f(S)$$<br>
which follows from an inductive application of the marginal gains definition. Thus,</p>
<p>$$\mathbb{E}[\Delta(a|A)] \ge \frac{1 - \epsilon}{k}(f(A^*) - f(A)),$$<br>
$$\mathbb{E}[f(A_{i+1}) - f(A_i)] \ge \frac{1 - \epsilon}{k}\mathbb{E}[f(A^*) - f(A_i)].$$</p>
<p>At the beginning, $f(A_0) = f(\emptyset) = 0$. In each iteration, the remaining difference between $f(A)$ and $f(A^*)$ is reduced by at least a $\frac{1 - \epsilon}{k}$ fraction in expectation. Thus, after $k$ iterations,<br>
$$<br>
\begin{align*}<br>
\mathbb{E}[f(A_k)] &amp;\ge f(A^*) \sum_{i=1}^k \left(\frac{1 - \epsilon}{k}\right)^i\\<br>
&amp;= f(A^*) \left(\frac{1 - \left(\frac{1 - \epsilon}{k}\right)^k}{1 - \frac{1 - \epsilon}{k}}\right)\\<br>
&amp;= f(A^*)<br>
\end{align*}<br>
$$</p>

                        </div>
                    </div>
                
            </div>
            <div id="body-right">
                <div id="categories">
    <div class="category">
        <a href="/all">All</a><br>
    </div>
    <div class="category">
        <a href="/blog">Blog</a><br>
    </div>
    <div class="category">
        <a href="/education">Education</a><br>
    </div>
    <div class="category">
	<a href="/fiction">Fiction</a><br>
    </div>
    <div class="category">
        <a href="/professional">Professional</a><br>
    </div>
    <div class="category">
        <a href="/research">Research</a><br>
    </div>
    <div class="category">
        <a href="/writings">Writings</a><br>
    </div>
</div>

                <nav>
     
    <div id="link">
        
        <a href="/assets/docs/cv.pdf">
            <img src="/assets/icons/pdf.png" class="textsize-image" alt="CV">
        </a>
        
        <!--
            <a href="/assets/docs/cv.pdf"><small>CV</small></a>
        -->
     
    <div id="link">
        
        <a href="https://github.com/jiwonac/">
            <img src="/assets/icons/github.png" class="textsize-image" alt="Github">
        </a>
        
        <!--
            <a href="https://github.com/jiwonac/"><small>Github</small></a>
        -->
     
    <div id="link">
        
        <a href="mailto:blog@jiwonc.net">
            <img src="/assets/icons/email.png" class="textsize-image" alt="Email">
        </a>
        
        <!--
            <a href="mailto:blog@jiwonc.net"><small>Email</small></a>
        -->
     
    <div id="link">
        
        <a href="https://www.linkedin.com/in/jiwonchang">
            <img src="/assets/icons/linkedin.png" class="textsize-image" alt="LinkedIn">
        </a>
        
        <!--
            <a href="https://www.linkedin.com/in/jiwonchang"><small>LinkedIn</small></a>
        -->
     
    <div id="link">
        
        <a href=" https://www.goodreads.com/jiwonac">
            <img src="/assets/icons/book.png" class="textsize-image" alt="GoodReads">
        </a>
        
        <!--
            <a href=" https://www.goodreads.com/jiwonac"><small>GoodReads</small></a>
        -->
     
    <div id="link">
        
        <a href="https://jiwonc.net/feed.xml">
            <img src="/assets/icons/rss.png" class="textsize-image" alt="RSS">
        </a>
        
        <!--
            <a href="https://jiwonc.net/feed.xml"><small>RSS</small></a>
        -->
    
</nav>
            </div>
        </div>
        <footer>
    <div>
        <small>© Jiwon Chang 2023</small>
    </div>
</footer>
    </div>
</body>